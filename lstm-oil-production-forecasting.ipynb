{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5510a0",
   "metadata": {},
   "source": [
    "# LSTM PROJECT TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cef394",
   "metadata": {},
   "source": [
    "## PROJECT SUMMARY\n",
    "Predicting future oil production rates for 5 oil wells\n",
    "\n",
    "Using the contents of the entire workbook create a 2 paragraph project introduction for ... project. The first paragraph should describe what the project is about and the base LSTM model being used. The second paragraph gives a brief summary of the steps that will be taken to analyze, train, and evaluate the model including EDA, training, testing, hyperparameter tuning, and comparing the base model to the .......... models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7cbda7",
   "metadata": {},
   "source": [
    "## DATA SOURCE & IMPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df1281",
   "metadata": {},
   "source": [
    "#### Data Source\n",
    "\n",
    "- The datasets used in this project are a combination of production data for five crude oil wells and commodity pricing data.\n",
    "- **Production Data**\n",
    "  - The historical production data has been provided directly by a small Canadian oil and gas producer.  \n",
    "  - *Reference*: Private Operator (Anonymous). (2025). Internal oil well production dataset: Five wells in Western Canada.\n",
    "- **Commodity Pricing Data**\n",
    "  - Historical crude oil and natural gas commodity pricing data is downloaded from a trusted Canadian petroleum reserve evaluator.\n",
    "  - *Reference*: McDaniel & Associates Consultants Ltd. (2025, October 1). Price forecasts [Web page]. https://mcdan.com/price-forecasts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dff86b",
   "metadata": {},
   "source": [
    "#### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core analysis, visualization, and statistical libraries used throughout the notebook\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from pathlib import Path\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from io import StringIO\n",
    "from textwrap import fill\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "\n",
    "# Silence warnings\n",
    "alt.data_transformers.disable_max_rows()\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "# Set seeds\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# Define model constants\n",
    "SCALING_FACTOR = 1e-6 # Log transform scaling factor to ensure zero values are handled correctly\n",
    "SPLIT_RATIO = 0.8     # Train-test or train-validation split ratio\n",
    "EPOCHS = 100          # Number of epochs to train models over\n",
    "BATCH_SIZE = 1        # Standard LSTM batch size\n",
    "PATIENCE = 10         # Early stopping level of patience\n",
    "N_TRIALS = 20         # Number of hyperparameter tuning trails to conduct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5b33a",
   "metadata": {},
   "source": [
    "#### Import Production Data\n",
    "- Production data has been provided directly from the oil company in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import production dataset file (as provided by the private oil company)\n",
    "prod_df = pd.read_csv('prod.csv')\n",
    "\n",
    "# Convert date column to datetime\n",
    "prod_df['date'] = pd.to_datetime(prod_df['date'])\n",
    "\n",
    "# Display production dataframe\n",
    "prod_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4ef68",
   "metadata": {},
   "source": [
    "#### Import Commodity Pricing Data\n",
    "- Commodity price data in `json` format is extracted directly from the McDaniel's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the commodity price json file and convert it into a dataframe\n",
    "def import_price_data(url):\n",
    "    response = requests.get(url, headers={'User-Agent': UserAgent().random})\n",
    "\n",
    "    data = json.loads(response.content)\n",
    "\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        date = entry['date']\n",
    "        row = {'date': date}\n",
    "        for item in entry['values']:\n",
    "            base_key = item['key'].replace(' Forecast', '').strip()\n",
    "            row[f'{base_key}'] = item['usd']\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "# Define the url links to the pricing data\n",
    "oil_url = 'https://mcdan.com/forecast/data/oil.json'\n",
    "gas_url = 'https://mcdan.com/forecast/data/gas.json'\n",
    "\n",
    "# Create oil and natural gas pricing datasets\n",
    "oil_prices_df = import_price_data(oil_url)\n",
    "gas_prices_df = import_price_data(gas_url)\n",
    "\n",
    "# Define prod_df min/max dates\n",
    "prod_min_date = prod_df['date'].min()\n",
    "prod_max_date = prod_df['date'].max()\n",
    "\n",
    "# Filter oil_prices_df and gas_prices_df by prod_min_date and prod_max_date\n",
    "oil_prices_df = oil_prices_df[(oil_prices_df['date']>=prod_min_date) & (oil_prices_df['date']<=prod_max_date)]\n",
    "gas_prices_df = gas_prices_df[(gas_prices_df['date']>=prod_min_date) & (gas_prices_df['date']<=prod_max_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c56d1",
   "metadata": {},
   "source": [
    "#### Merge Production & Pricing Datasets\n",
    "- Create the final dataset used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5303ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the prod_df, oil_prices_df, and gas_prices_df\n",
    "df = prod_df.merge(oil_prices_df).merge(gas_prices_df)\n",
    "\n",
    "# Display combined dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a75a9",
   "metadata": {},
   "source": [
    "## DATA SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2d920",
   "metadata": {},
   "source": [
    "#### Dataset Shape & Data Types\n",
    "- Investigating the number of rows and columns contained in the dataset, and the types of data found in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "(rows, cols) = df.shape\n",
    "print(f'The dataset has {rows:,} rows and {cols} columns.\\n')\n",
    "\n",
    "# Dataset data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518422d4",
   "metadata": {},
   "source": [
    "#### Feature Descriptions\n",
    "- The dataset contains a combination of production rates and commodity pricing streams.\n",
    "\n",
    "Feature | Description | Units | Data Type\n",
    "---|---|---|---\n",
    "**well_name** | Name of the producing well | | Categorical\n",
    "**date** | First day of each month of production | dd/mm/yyy | Datetime\n",
    "**production_hours** | Number of producing hours a well is on production in a month | hours | Numeric\n",
    "**oil_producing_daily_avg (bbl/day)** | Monthly average of the daily oil production rate | bbl/d | Numeric\n",
    "**gas_producing_daily_avg (mcf/day)** | Monthly average of the daily natural gas production rate | mcf/d | Numeric\n",
    "**water_producing_daily_avg (bbl/day)** | Monthly average of the daily water production rate | bbl/d | Numeric\n",
    "**WTI** | Monthly average of the West Texas Intermediate crude oil benchmark price | US $/bbl | Numeric\n",
    "**BRENT** | Monthly average of the Brent crude oil benchmark price | US $/bbl | Numeric\n",
    "**EDM** | Monthly average of the Edmonton Light Sweet crude oil benchmark price | US $/bbl | Numeric\n",
    "**WCS** | Monthly average of the Western Canadian Select crude oil benchmark price | US $/bbl | Numeric\n",
    "**HH** | Monthly average of the Henry Hub natural gas benchmark price | US $/mcf | Numeric\n",
    "**AECO** | Monthly average of the AECO natural gas benchmark price | US $/mcf | Numeric\n",
    "**STA2** | Monthly average of the BC Station 2 natural gas benchmark price | US $/mcf | Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b0c69",
   "metadata": {},
   "source": [
    "> - For this project **the target variable is the `oil_producing_daily_avg (bbl/day)`** column at time $t+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcbc4a",
   "metadata": {},
   "source": [
    "## EDA\n",
    "The EDA section presents monthly production trends by well, blends in macro indicators, and highlights shut-in periods to reveal The EDA section traces monthly production trends for every well, overlaying macro pricing indicators and shut-in periods to surface operational cycles, sudden changes, and downtime. It also frames the upcoming train/test split within that historical context, so the LSTM preparation later clearly ties back to the exploratory insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d5fe8",
   "metadata": {},
   "source": [
    "#### Frequency of Categorical Columns\n",
    "- Each well has produced for a different length of time, where each row in the dataset represents a month of production.\n",
    "- It would be interesting to visualize the difference between the first and last day of production by counting the row occurrences in the `well_name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize months on production\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    alt.X('well_name:N').title(None),\n",
    "    alt.Y('count():Q').title(None),\n",
    "    alt.Color('well_name').scale(scheme='viridis')\n",
    ").properties(\n",
    "    height = 300,\n",
    "    width = 500,\n",
    "    title=alt.Title('Months on Production by Well', fontSize=20, align='right'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e7f81",
   "metadata": {},
   "source": [
    "#### Distributions of Numeric Columns\n",
    "- These are used to visualize how numeric columns are distributed, and to observe any patterns or outliers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a set of histograms for a subset of columns\n",
    "def plot_numeric_histograms(input_df, subset, chart_title, col_count):\n",
    "    # Melt the numeric columns into one column\n",
    "    df_melt = input_df[subset].melt(var_name='feature', value_name='value')\n",
    "\n",
    "    # Create a base Altair histogram chart\n",
    "    base_chart = alt.Chart(df_melt).mark_bar(opacity=1, binSpacing=0).encode(\n",
    "        alt.X('value:Q').axis(title=None).bin(maxbins=50),\n",
    "        alt.Y('count():Q').axis(title=None).stack(None),\n",
    "        color = alt.Color('feature:N').scale(scheme='viridis').sort(df.columns).legend(None)\n",
    "    ).properties(\n",
    "        width=750 / col_count,\n",
    "        height=150\n",
    "    )\n",
    "\n",
    "    # Display a histogram for each numeric_columns\n",
    "    chart = alt.ConcatChart(\n",
    "        title=alt.Title(f'{chart_title}', fontSize=20),\n",
    "        concat=[base_chart.transform_filter(alt.datum.feature == value).properties(title=value) for value in subset],\n",
    "        columns=col_count,\n",
    "    ).configure_title(\n",
    "        fontSize=10\n",
    "    ).resolve_axis(\n",
    "        x='independent',\n",
    "        y='independent'\n",
    "    ).resolve_scale(\n",
    "        x='independent', \n",
    "        y='independent'\n",
    "    )\n",
    "\n",
    "    return chart\n",
    "\n",
    "# Define numeric feature columns\n",
    "numeric_columns = [col for col in df.columns.to_list() if col not in ['well_name','date']]\n",
    "\n",
    "# Plot numeric histograms\n",
    "plot_numeric_histograms(df, numeric_columns, 'Numeric Columns Data Distributions', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368aecf1",
   "metadata": {},
   "source": [
    "> - The `oil_producing_daily_avg (bbl/day)` and `water_producing_daily_avg (bbl/day)` columns seem to be somewhat exponentially distributed, whereas the `gas_producing_daily_avg (mcf/day)` column has more variation and looks to be a skewed distribution.\n",
    "> - The crude oil commodity prices (`WTI`, `BRENT`, `EDM`, `WCS`) all seem to have bi-modal distributions indicating that those prices may experience periods of high and low price environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65ea40",
   "metadata": {},
   "source": [
    "#### Numeric Columns Pairplot\n",
    "- This pairplot visualizes the scatterplots and histograms that reveals how production metrics and pricing indicators move together across wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only numeric features\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "pairgrid_df = df[numeric_cols].dropna()\n",
    "\n",
    "# Set theme\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk', font_scale=0.9)\n",
    "sns.set_palette('viridis')\n",
    "viridis_colors = sns.color_palette('viridis_r', 5)\n",
    "hist_color = viridis_colors[2]\n",
    "scatter_color = viridis_colors[-1]\n",
    "kde_cmap = sns.color_palette('viridis', as_cmap=True)\n",
    "\n",
    "# Build the PairGrid with histograms on the diagonal, scatterplots above, and KDE contours below\n",
    "pairgrid = sns.PairGrid(pairgrid_df, diag_sharey=False)\n",
    "pairgrid.map_diag(sns.histplot, color=hist_color, edgecolor='white', linewidth=1.0)\n",
    "pairgrid.map_upper(sns.scatterplot, color=scatter_color, s=25, alpha=0.7, edgecolor='white', linewidth=0.3)\n",
    "pairgrid.map_lower(sns.kdeplot, fill=True, thresh=0.04, levels=5, cmap=kde_cmap, linewidths=0.8)\n",
    "\n",
    "# Rotate and wrap long axis labels\n",
    "for ax in pairgrid.axes.flatten():\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "    ax.tick_params(axis='y', labelrotation=0)\n",
    "    ax.set_xlabel(fill(ax.get_xlabel(), 12))\n",
    "    ax.set_ylabel(fill(ax.get_ylabel(), 12))\n",
    "\n",
    "# Remove extra whitespace and add an overall title\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "pairgrid.fig.suptitle('Numeric Feature Relationships', y=1.02)\n",
    "\n",
    "pairgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca033c",
   "metadata": {},
   "source": [
    "> - Obvious correlations are observed between the production volumes and production rates. This makes sense since they are just a transformation of each other (as described in the Feature Descriptions section).\n",
    "> - Correlations also exist among the crude oil pricing columns (`WTI`, `BRENT`, `EDM`, `WCS`). These are not unusual since most crude oils are priced relative to WTI. Similarly correlations are also observed for the natural gas prices between `AECO` and `STA2`. This is expected since both commodites are traded into similar markets.\n",
    "> - No other obvious correlations are observed among the other numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6539348",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "- The correlation matrix is a heatmap of pairwise correlations between production and pricing features, helping this analysis quickly spot which variables move together before selecting inputs for the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix across all numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk', font_scale=0.9)\n",
    "plt.figure(figsize=(14, 12))\n",
    "heatmap = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    cmap='viridis',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    annot=True,\n",
    "    fmt='.4f',\n",
    "    annot_kws={'size': 10},\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Correlation coefficient', 'shrink': 0.8},\n",
    "    square=True\n",
    ")\n",
    "\n",
    "# Resize the legend\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "cbar.set_label('Correlation coefficient', size=8)\n",
    "\n",
    "# Realign axis labels\n",
    "heatmap.set_title('Numeric Feature Correlation Matrix', pad=20)\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, ha='right', fontsize=8)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c44f30",
   "metadata": {},
   "source": [
    "> Correlations that were visually observed in the pairplot are confirmed in the correlation heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f949a",
   "metadata": {},
   "source": [
    "#### Production Time Series Plots\n",
    "- Oil and natural gas production rates typically exponentially decline over time so they are plotted on a semi-log plot over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64533a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for plot\n",
    "prod_rate_dict = {\n",
    "    'production_hours':'Hours On Production (hrs)',\n",
    "    'oil_producing_daily_avg (bbl/day)':'Oil Production (bbl/d)',\n",
    "    'gas_producing_daily_avg (mcf/day)':'Gas Production (mcf/d)',\n",
    "    'water_producing_daily_avg (bbl/day)':'Water Production (bbl/d)'\n",
    "}\n",
    "\n",
    "# Convert data into a dataframe for Altair plotting\n",
    "prod_rate_chart_df = (\n",
    "    df[['well_name', 'date'] + [k for k in prod_rate_dict.keys()]]\n",
    "        .rename(prod_rate_dict, axis=1)\n",
    "        .replace({0:np.nan})\n",
    "        .melt(id_vars=['well_name', 'date'], value_vars=[v for v in prod_rate_dict.values()])\n",
    ")\n",
    "\n",
    "# Create plot of log-scaled time series for oil rate, gas rate, water rate, and hours on for each well\n",
    "alt.Chart(prod_rate_chart_df).mark_line().encode(\n",
    "    alt.X('date:T').title(None),\n",
    "    alt.Y('value:Q').scale(type='log').title('Production Rate (bb/d or mcf/d)'),\n",
    "    alt.Color('variable:N').scale(domain=[v for v in prod_rate_dict.values()], range=['grey', '#2ca02c', '#d62728', 'steelblue']),\n",
    "    alt.Row('well_name:N').title(None),\n",
    "    alt.Tooltip(['value:Q'])\n",
    ").properties(\n",
    "    height = 200,\n",
    "    width = 800,\n",
    "    title=alt.Title('Oil, Gas, and Water Production Rates Over Time', fontSize=20, anchor='start')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e73591",
   "metadata": {},
   "source": [
    "> Comments about production plots..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10769c",
   "metadata": {},
   "source": [
    "#### Oil Production Rate Moving Averages\n",
    "- Visualizing the 12 month, 24, and 36 months oil production rate moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb09bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for moving-average smoothing\n",
    "ma_chart_df = df[['well_name', 'date', 'oil_producing_daily_avg (bbl/day)']].replace({0:np.nan})\n",
    "\n",
    "# Define the moving-average windows\n",
    "window_specs = [\n",
    "    (12, '12-month MA'),\n",
    "    (24, '24-month MA'),\n",
    "    (36, '36-month MA'),\n",
    "]\n",
    "\n",
    "# Define moving-average line colors\n",
    "window_colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "color_scale = alt.Scale(\n",
    "    domain=[label for _, label in window_specs],\n",
    "    range=window_colors\n",
    ")\n",
    "\n",
    "# Build base MA chart\n",
    "base = alt.Chart(ma_chart_df).encode(\n",
    "    alt.X('date:T').title('Date')\n",
    ")\n",
    "\n",
    "# Plot oil production line chart\n",
    "raw_line = base.mark_line(color='green', opacity=0.2, size=5).encode(\n",
    "    alt.Y('oil_producing_daily_avg (bbl/day):Q').title('Oil producing daily avg (bbl/day)').scale(type='log')\n",
    ")\n",
    "\n",
    "# Create each moving-average line charts\n",
    "ma_layers = []\n",
    "for window, label in window_specs:\n",
    "    ma_layer = base.transform_window(\n",
    "        rolling_mean='mean(oil_producing_daily_avg (bbl/day))',\n",
    "        frame=[-(window - 1), 0],\n",
    "        groupby=['well_name'],\n",
    "        sort=[alt.SortField('date')]\n",
    "    ).transform_calculate(\n",
    "        window_label=f\"'{label}'\"\n",
    "    ).mark_line(size=3, opacity=0.75, strokeDash=[10,4]).encode(\n",
    "        y='rolling_mean:Q',\n",
    "        color=alt.Color('window_label:N').scale(color_scale).title('Moving Average')\n",
    "    ).properties(\n",
    "        height = 200,\n",
    "        width = 800,\n",
    "    )\n",
    "    ma_layers.append(ma_layer)\n",
    "\n",
    "# Layer the raw_line and ma_layer charts, facet by well\n",
    "layered_ma_chart = alt.layer(*ma_layers, raw_line).facet(\n",
    "    row=alt.Row('well_name:N', title='Well')\n",
    ").properties(\n",
    "    title=alt.Title('12, 24, and 36 Month Moving Averages of the Oil Production Rate (bbl/d)', fontSize=20, anchor='start')\n",
    ")\n",
    "\n",
    "layered_ma_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a0bea",
   "metadata": {},
   "source": [
    "> The 12, 24, and 36-month moving average curves show that every well’s oil production rate follows a steady downward slope, with the longer windows smoothing out the month-to-month volatility. Large negative spikes mark occasional shut-ins or outages, but the multi-year trend remains a gradual decline across all wells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f6904",
   "metadata": {},
   "source": [
    "#### Oil Production Percent Change\n",
    "- The month-to-month percent change chart tracks how each well’s production jumps or drops from one month to the next, helping the analysis identify volatility bursts or shut-in events to account for when modeling the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09121ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute month-over-month percent change in oil production for each well\n",
    "pct_change_df = df[['well_name', 'date', 'oil_producing_daily_avg (bbl/day)']].copy()\n",
    "pct_change_df = pct_change_df.sort_values(['well_name', 'date'])\n",
    "pct_change_df['pct_change'] = pct_change_df.groupby('well_name')['oil_producing_daily_avg (bbl/day)'].pct_change() * 100\n",
    "pct_change_df = pct_change_df.dropna(subset=['pct_change'])\n",
    "pct_change_df['pct_change'] = pct_change_df['pct_change'].clip(-100, 100)\n",
    "\n",
    "# Define positive/negative color scale\n",
    "color_scale = alt.Scale(domain=['Increase', 'Decrease'], range=['#2ca02c', '#d62728'])\n",
    "\n",
    "# Plot a the percent change chart\n",
    "pct_change_chart = alt.Chart(pct_change_df).mark_bar().encode(\n",
    "    x=alt.X('date:T', title='Date'),\n",
    "    y=alt.Y('pct_change:Q', title='Month-to-month % change', scale=alt.Scale(domain=[-100, 100])),\n",
    "    color=alt.Color(\n",
    "        'change_direction:N',\n",
    "        scale=color_scale,\n",
    "        legend=alt.Legend(title='Direction')\n",
    "    ),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('well_name:N', title='Well'),\n",
    "        alt.Tooltip('date:T', title='Date'),\n",
    "        alt.Tooltip('pct_change:Q', title='% change', format='.2f')\n",
    "    ]\n",
    ").transform_calculate(\n",
    "    change_direction=\"datum.pct_change >= 0 ? 'Increase' : 'Decrease'\"\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=800\n",
    ").facet(\n",
    "    row=alt.Row('well_name:N', title='Well')\n",
    ").resolve_scale(\n",
    "    y='shared'\n",
    ").properties(\n",
    "    title=alt.Title('Month-to-Month Percent Change of the Oil Production Rate', fontSize=20, anchor='start', subtitle='(capped at +/-100%)')\n",
    ")\n",
    "\n",
    "pct_change_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ef909",
   "metadata": {},
   "source": [
    "> edit this....\n",
    "> - More negative rather than positive percent changes are prevalent for each well. This makes sense because the production rate typically consistently declines over time.\n",
    "> - It would be interesting to study the green bars to understand what occurred to cause an increase in oil production (which is a good thing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c713a",
   "metadata": {},
   "source": [
    "#### Seasonal Decomposition Plots\n",
    "- Seasonal decomposition charts break each time series into trend, seasonal, and residual components, revealing long-term decline, recurring seasonal patterns, and irregular shocks separately.\n",
    "- In this analysis they help determine whether seasonal patterns or residual noise should be modeled explicitly or if the LSTM can focus mostly on the trend and notable anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb238481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect seasonal decomposition components for every well\n",
    "decomposition_records = []\n",
    "value_col = 'oil_producing_daily_avg (bbl/day)'\n",
    "\n",
    "for well_name, group in df[['well_name', 'date', value_col]].dropna(subset=[value_col]).sort_values('date').groupby('well_name'):\n",
    "    well_series = group.set_index('date')[value_col]\n",
    "    monthly_series = well_series.resample('MS').mean().interpolate(limit_direction='both')\n",
    "\n",
    "    if len(monthly_series.dropna()) < 24:\n",
    "        continue\n",
    "\n",
    "    decomposition = seasonal_decompose(\n",
    "        monthly_series,\n",
    "        model='additive',\n",
    "        period=12,\n",
    "        extrapolate_trend='freq'\n",
    "    )\n",
    "\n",
    "    component_map = {\n",
    "        'Observed': decomposition.observed,\n",
    "        'Trend': decomposition.trend,\n",
    "        'Seasonal': decomposition.seasonal,\n",
    "        'Residual': decomposition.resid\n",
    "    }\n",
    "\n",
    "    for component_name, component_values in component_map.items():\n",
    "        component_df = component_values.reset_index()\n",
    "        component_df.columns = ['date', 'value']\n",
    "        component_df['component'] = component_name\n",
    "        component_df['well_name'] = well_name\n",
    "        decomposition_records.append(component_df)\n",
    "\n",
    "# Combine component data and drop rows created by leading/trailing NaNs\n",
    "seasonal_decomposition_df = pd.concat(decomposition_records, ignore_index=True)\n",
    "seasonal_decomposition_df = seasonal_decomposition_df.dropna(subset=['value'])\n",
    "component_order = ['Observed', 'Trend', 'Seasonal', 'Residual']\n",
    "\n",
    "# Plot the observed, trend, seasonal, and residual series for each well in a faceted grid\n",
    "seasonal_decomposition_chart = alt.Chart(seasonal_decomposition_df).mark_line(color='#2ca02c').encode(\n",
    "    x=alt.X('date:T').title(None),\n",
    "    y=alt.Y('value:Q').title(None),\n",
    "    row=alt.Row('component:N', sort=component_order, title=None),\n",
    "    column=alt.Column('well_name:N', title='Well'),\n",
    "    color=alt.Color('well_name:N', title='Well', scale=alt.Scale(scheme='viridis'))\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ").properties(\n",
    "    height=100,\n",
    "    width=200,\n",
    "    title=alt.Title('Seasonal Decomposition of the Oil Production Rate', fontSize=20, anchor='start')\n",
    ")\n",
    "\n",
    "seasonal_decomposition_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de02fc",
   "metadata": {},
   "source": [
    "> The seasonal decomposition charts show a steep decline in each well’s long-term trend followed by a flat tail, while the seasonal component oscillates annually with consistent amplitude—**clear evidence of recurring 12-month seasonality**. Residuals remain relatively small after the initial drop, suggesting most structure is captured by the trend and seasonal terms. LSTM's inherently handle seasonality, but if model fits are insufficient then we could consider applying temporal differencing to account for these cyclical effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e64373",
   "metadata": {},
   "source": [
    "#### Autocorrelation Plot\n",
    "- The autocorrelation plot shows how each well’s production correlates with its past values at different lags, helping this analysis gauge how far back the LSTM needs to look to capture meaningful temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by well\n",
    "well_groups = (\n",
    "    df[['well_name', 'date', 'oil_producing_daily_avg (bbl/day)']]\n",
    "    .dropna(subset=['oil_producing_daily_avg (bbl/day)'])\n",
    "    .sort_values('date')\n",
    "    .groupby('well_name')\n",
    ")\n",
    "\n",
    "# Define horizon to analyze autocorrelation over\n",
    "horizon = 36\n",
    "\n",
    "# Create ACF datasets\n",
    "acf_records = []\n",
    "for well_name, group in well_groups:\n",
    "    monthly_series = group.set_index('date')[value_col].resample('MS').mean().interpolate(limit_direction='both')\n",
    "    acf_values = acf(monthly_series, nlags=horizon, fft=False, missing='drop')\n",
    "    for lag in range(1, horizon + 1):\n",
    "        acf_records.append({\n",
    "            'well_name':     well_name,\n",
    "            'lag':           lag,\n",
    "            'acf':           acf_values[lag],\n",
    "            'horizon_label': f\"{horizon}-month\"\n",
    "        })\n",
    "\n",
    "# Plot ACF chart\n",
    "acf_df = pd.DataFrame(acf_records)\n",
    "acf_chart = alt.Chart(acf_df).mark_bar().encode(\n",
    "    x=alt.X('lag:O', title='Lag (months)', axis=alt.Axis(values=[6, 12, 18, 24, 30, 36])),\n",
    "    y=alt.Y('acf:Q', title='Autocorrelation', scale=alt.Scale(domain=[-1, 1])),\n",
    "    column=alt.Column('well_name:N', title=None),\n",
    "    row=alt.Row('horizon_label:N', sort=[f\"36-month\"], title='Horizon'),\n",
    "    color=alt.condition(alt.datum.acf >= 0, alt.value('#1f77b4'), alt.value('#d62728'))\n",
    ").resolve_scale(\n",
    "    y='shared'\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=200,\n",
    "    title=alt.Title('36 month Autocorrelation of the Oil Production Rate', fontSize=20, anchor='start')\n",
    ")\n",
    "\n",
    "acf_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b380d",
   "metadata": {},
   "source": [
    "> Autocorrelation stays strongly positive through roughly the first 12–18 months for every well, confirming that recent production levels heavily influence the next year’s output. Beyond about 24 months the correlations decay to near zero or slightly negative, suggesting the model can safely prioritize shorter lookback windows without losing much signal. With this knowledge **I'll consider testing look back windows of 12, 24, and 36 months for the LSTM models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c6186",
   "metadata": {},
   "source": [
    "## DATA CLEANING & PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eadc4a",
   "metadata": {},
   "source": [
    "#### Adding Columns Required for Modeling\n",
    "- A `months_on` column is added that calculates the number of months the well has been on production since its first month of production.\n",
    "- A set of dummy variable columns are created that define general states of the oil and gas industry over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a months_on column which is the number of months a well has produced since it was first put on production\n",
    "df['months_on'] = df.groupby('well_name')['date'].transform(lambda s: s.rank(method='first').astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns that identify key events affecting the oil and gas industry\n",
    "indicator_windows = {\n",
    "    'middle_instability_tightening_supply': [('2009-01-01', '2014-08-01')],\n",
    "    'opec_oversupply_during_downturn':      [('2014-09-01', '2016-02-01')],\n",
    "    'opec_undersupply_during_recovery':     [('2016-03-01', '2020-02-01')],\n",
    "    'covid':                                [('2020-03-01', '2022-06-01')],\n",
    "    'ukraine_war':                          [('2022-02-01', '2025-11-01')]\n",
    "}\n",
    "\n",
    "# Create date index\n",
    "min_prod_date = df['date'].min().to_period('M').to_timestamp()\n",
    "date_index = pd.date_range(start=min_prod_date, end='2025-11-01', freq='MS')\n",
    "\n",
    "# Create & populate dummy variables\n",
    "dummy_df = pd.DataFrame({'date': date_index})\n",
    "for indicator, windows in indicator_windows.items():\n",
    "    dummy_df[indicator] = 0\n",
    "    for start, end in windows:\n",
    "        start_ts = pd.Timestamp(start)\n",
    "        end_ts = pd.Timestamp(end)\n",
    "        mask = (dummy_df['date'] >= start_ts) & (dummy_df['date'] <= end_ts)\n",
    "        dummy_df.loc[mask, indicator] = 1\n",
    "\n",
    "# Merge dummy_df with the dataset\n",
    "df = df.merge(dummy_df, how='left')\n",
    "\n",
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30c253",
   "metadata": {},
   "source": [
    "#### Scale Data\n",
    "- Models are trained using the concatenation of all the well's `X_train` and `y_train` datasets, and then individually predicted and tested using each wells `X_test` and `y_test` datasets.\n",
    "- Because of this I scale the entire dataset's numeric features to a mean of 0 and standard deviation of 1 to improve generality of the LSTM models.\n",
    "- Numeric features that are scaled are:\n",
    "  - `production_hours`\n",
    "  - `oil_producing_daily_avg (bbl/day)`\n",
    "  - `gas_producing_daily_avg (mcf/day)`\n",
    "  - `water_producing_daily_avg (bbl/day)`\n",
    "  - `WTI`\n",
    "  - `BRENT`\n",
    "  - `EDM`\n",
    "  - `WCS`\n",
    "  - `HH`\n",
    "  - `AECO`\n",
    "  - `STA2`\n",
    "  - `months_on`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the StandardScaler so X features can be scaled later\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.iloc[:, 2:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587f0a3",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "- LSTM models expect a 3-dimensional dataframe as input with a shape of: ($\\textit{samples}$, $\\textit{look back}$, $\\textit{number of features}$).\n",
    "  - Where:\n",
    "    - $\\textit{samples}$ = are the number of unique time slices generated from the set of features $X$\n",
    "    - $\\textit{look back}$ = is the length of each sample (measured in months)\n",
    "    - $\\textit{number of features}$ = are the number of feature variables in the input matrix $X$\n",
    "- The `create_dataset` function below creates the 3-dimensional LSTM inputs for a given input dataset\n",
    "  - The target values $y$ are the `oil_producing_daily_avg (bbl/day)` values taken at time $t+1$\n",
    "  - The feature matrix $X$ contains 17 features:\n",
    "    - `production_hours`\n",
    "    - `oil_producing_daily_avg (bbl/day)`\n",
    "    - `gas_producing_daily_avg (mcf/day)`\n",
    "    - `water_producing_daily_avg (bbl/day)`\n",
    "    - `WTI`\n",
    "    - `BRENT`\n",
    "    - `EDM`\n",
    "    - `WCS`\n",
    "    - `HH`\n",
    "    - `AECO`\n",
    "    - `STA2`\n",
    "    - `months_on`\n",
    "    - `middle_instability_tightening_supply`\n",
    "    - `opec_oversupply_during_downturn`\n",
    "    - `opec_undersupply_during_recovery`\n",
    "    - `covid`\n",
    "    - `ukraine_war`\n",
    "- $X$ and $y$ are split into training and testing sets, where:\n",
    "  - the first 80% of the time series is used as the training set, and \n",
    "  - the last 20% of the time series is used as the testing set\n",
    "- Scaling is applied where:\n",
    "  - The $X$ numeric values are scaled using the standard scaler mentioned above\n",
    "  - $y$ is log transformed (with a scaling factor of 1e-6 added to avoid errors when $y=0$)\n",
    "- The `X_train` and `y_train` sets for each well are stacked into one so the model can be trained on one complete training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale and split a dataset into X_train, X_test, y_train, y_test\n",
    "def create_train_test_split(dataset, look_back, scaler):\n",
    "    # Drop date and well_name columns\n",
    "    dataset = dataset[[col for col in dataset.columns if col not in ['date', 'well_name']]]\n",
    "\n",
    "    # Split into X & y\n",
    "    X = dataset.iloc[0:(len(dataset)-1), :].to_numpy()\n",
    "    y = dataset.iloc[1:, 1]\n",
    "\n",
    "    # Scale the numeric features in X\n",
    "    X[:,:12] = scaler.transform(X[:,:12])\n",
    "\n",
    "    # Log transform y\n",
    "    y_log = np.log(y + SCALING_FACTOR).to_numpy()\n",
    "\n",
    "    # Split into train & test sets\n",
    "    train_idx = int(len(X) * SPLIT_RATIO)\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "    # Create X_train & y_train datasets\n",
    "    for i in range(train_idx - look_back + 1):\n",
    "        X_train_sample = X[i:(i + look_back), :]\n",
    "        X_train.append(X_train_sample)\n",
    "        y_train.append(y_log[i + look_back - 1])\n",
    "    X_train = np.array(X_train) # X_train.shape = (train_idx - look_back + 1, look_back, n_features)\n",
    "    y_train = np.array(y_train) # y_train.shape = (train_idx - look_back + 1, )\n",
    "\n",
    "    # Create X_test & y_test datasets\n",
    "    for i in range(train_idx - look_back + 1, len(X) - look_back + 1):\n",
    "        X_test_sample = X[i:(i+look_back), :]\n",
    "        X_test.append(X_test_sample)\n",
    "        y_test.append(y_log[i + look_back - 1])\n",
    "    X_test = np.array(X_test) # X_test.shape = (len(X) - train_idx, look_back, n_features)\n",
    "    y_test = np.array(y_test) # y_test.shape = (len(y) - train_idx, )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to loop over look_back windows and well_names to create a dictionary of train-test datasets that will be used for model training\n",
    "def create_train_test_dict(df, scaler):\n",
    "    train_test_dict = {}\n",
    "    for look_back in [12, 24, 36]:\n",
    "        for well_name in df['well_name'].unique():\n",
    "\n",
    "            # Ensure nested levels exist\n",
    "            if well_name not in train_test_dict:\n",
    "                train_test_dict[well_name] = {}\n",
    "            if look_back not in train_test_dict[well_name]:\n",
    "                train_test_dict[well_name][look_back] = {}\n",
    "\n",
    "            # Extract dataset\n",
    "            dataset = df[df['well_name'] == well_name]\n",
    "            X_train, X_test, y_train, y_test = create_train_test_split(dataset, look_back, scaler)\n",
    "\n",
    "            # Create entires\n",
    "            train_test_dict[well_name][look_back] = {\n",
    "                'X_train': X_train,\n",
    "                'X_test':  X_test,\n",
    "                'y_train': y_train,\n",
    "                'y_test':  y_test\n",
    "            }\n",
    "            \n",
    "    return train_test_dict\n",
    "\n",
    "# Generate datasets dictionary\n",
    "train_test_dict = create_train_test_dict(df, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shapes from train_test_dict for X_train and y_train for lookbacks = 12, 24, and 36 months\n",
    "print('X_train has shape (samples, look_back, n_features)\\ny_train has shape (samples, )')\n",
    "for look_back in [12, 24, 36]:\n",
    "    _x_train = np.concatenate([train_test_dict[well_name][look_back]['X_train'] for well_name in df['well_name'].unique()], axis=0)\n",
    "    _y_train = np.concatenate([train_test_dict[well_name][look_back]['y_train'] for well_name in df['well_name'].unique()], axis=0)\n",
    "    print(f'\\nCombined training dataset shapes for look_back = {look_back}:\\n-> X_train : {_x_train.shape}\\n-> y_train : {_y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94052634",
   "metadata": {},
   "source": [
    "## LSTM MODELING\n",
    "Describe base vanilla LSTM model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68566eea",
   "metadata": {},
   "source": [
    "#### Define LSTM Models\n",
    "- In this analysis I'll be evaluating 4 types of LSTM model. The first is a Vanilla LSTM, which I call my Base LSTM\n",
    "- The base model is used as a datum to compare to the other models to\n",
    "- The other models are the Bidirectional LSTM, Stacked LSTM, and CNN-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac1e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model instances\n",
    "def create_LSTM_model(model_type, n_nodes, look_back, learning_rate, dropout, n_features):\n",
    "    strategy = tf.distribute.MirroredStrategy() # Utilize GPUs\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        if model_type == 'base': # Base LSTM\n",
    "            model.add(LSTM(n_nodes, input_shape=(look_back, n_features)))\n",
    "        if model_type == 'bidirectional': # Bidirectional LSTM\n",
    "            model.add(Bidirectional(LSTM(n_nodes), input_shape=(look_back, n_features)))\n",
    "        if model_type == 'stacked': # Stacked LSTM\n",
    "            model.add(LSTM(n_nodes * 2, return_sequences=True, input_shape=(look_back, n_features)))\n",
    "            model.add(LSTM(n_nodes))\n",
    "        if model_type == 'cnn': # CNN-LSTM\n",
    "            model.add(Conv1D(filters=32, kernel_size=3, padding='causal', input_shape=(look_back, n_features)))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(LSTM(n_nodes))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b720f4",
   "metadata": {},
   "source": [
    "#### Fit Base LSTM Model\n",
    "- Fit an LSTM model using given X_train and y_train\n",
    "- Function generates:\n",
    "  - Train & test RMSE scores\n",
    "  - Train-validation loss plot (with 20% of the data used for validation)\n",
    "  - Train and test actual vs pred line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426615b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit base LSTM\n",
    "def fit_lstm(model):\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE) # Add early stopping to prevent overfitting\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=(1-SPLIT_RATIO),\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Create loss plot\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model Train Loss vs Validation Loss ({well_name})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Define base LSTM model inputs\n",
    "model_type = 'base'\n",
    "look_back = 12\n",
    "n_nodes = 4\n",
    "learning_rate = 0.001\n",
    "dropout = 0.1\n",
    "\n",
    "# Generate X_train and y_train datasets for the given n_nodes, look_back and learning_rate\n",
    "X_train = np.concatenate([train_test_dict[well_name][look_back]['X_train'] for well_name in train_test_dict.keys()], axis=0)\n",
    "y_train = np.concatenate([train_test_dict[well_name][look_back]['y_train'] for well_name in train_test_dict.keys()], axis=0)\n",
    "\n",
    "# Generate the base model instance and display model summary\n",
    "model = create_LSTM_model(model_type, n_nodes, look_back, learning_rate, dropout, X_train.shape[2])\n",
    "model.summary()\n",
    "\n",
    "# Fit the base model LSTM and display the loss plot\n",
    "history = fit_lstm(model)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c9cff",
   "metadata": {},
   "source": [
    "> Comments on loss plot ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bf018",
   "metadata": {},
   "source": [
    "#### Well 1 Base LSTM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict oil production from train-test datasets\n",
    "def predict_oil_production(train_test_dict, well_name, look_back, history):\n",
    "    X_train = train_test_dict[well_name][look_back]['X_train']\n",
    "    y_train = train_test_dict[well_name][look_back]['y_train']\n",
    "    X_test = train_test_dict[well_name][look_back]['X_test']\n",
    "    y_test = train_test_dict[well_name][look_back]['y_test']\n",
    "\n",
    "    # Make train-test predictions\n",
    "    trainPredict = history.model.predict(X_train, verbose=0)\n",
    "    testPredict = history.model.predict(X_test, verbose=0)\n",
    "\n",
    "    # Unscale target variables and predictions\n",
    "    y_train = np.exp(y_train) - SCALING_FACTOR\n",
    "    trainPredict = np.exp(trainPredict) - SCALING_FACTOR\n",
    "    y_test = np.exp(y_test) - SCALING_FACTOR\n",
    "    testPredict = np.exp(testPredict) - SCALING_FACTOR\n",
    "\n",
    "    # Calculate train-test RMSE\n",
    "    print('\\n============================================================================')\n",
    "    print(f'Root Mean Squared Error Scores ({well_name})')\n",
    "    print('============================================================================\\n')   \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, trainPredict))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, testPredict))\n",
    "    print(f'Train RMSE: {train_rmse:,.1f}')\n",
    "    print(f'Test RMSE: {test_rmse:,.1f}\\n')\n",
    "\n",
    "    # Actual vs predicted train-test line charts\n",
    "    print('\\n============================================================================')\n",
    "    print(f'Actual vs Predicted Plots ({well_name})')\n",
    "    print('============================================================================\\n')   \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 6), sharey=True)\n",
    "    ax1.plot(y_train, label='y_train Actual')\n",
    "    ax1.plot(trainPredict, label='y_train Predicted')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('Oil Production Rate (bbl/d)')\n",
    "    ax1.set_xlabel('Months')\n",
    "    ax2.plot(y_test, label='y_test Actual')\n",
    "    ax2.plot(testPredict, label='y_test Predicted')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Months')\n",
    "    fig.suptitle(f'Oil Production Rate Train/Test Actual vs Predicted ({well_name})')\n",
    "\n",
    "# Fit LSTM model and predict for well_1\n",
    "predict_oil_production(train_test_dict, 'well_1', look_back, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f53a9",
   "metadata": {},
   "source": [
    "#### Well 2 Base LSTM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LSTM model and predict for well_2\n",
    "predict_oil_production(train_test_dict, 'well_2', look_back, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab28f2",
   "metadata": {},
   "source": [
    "#### Well 3 Base LSTM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LSTM model and predict for well_3\n",
    "predict_oil_production(train_test_dict, 'well_3', look_back, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3011c5",
   "metadata": {},
   "source": [
    "#### Well 4 Base LSTM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LSTM model and predict for well_4\n",
    "predict_oil_production(train_test_dict, 'well_4', look_back, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a9741",
   "metadata": {},
   "source": [
    "#### Well 5 Base LSTM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LSTM model and predict for well_5\n",
    "predict_oil_production(train_test_dict, 'well_5', look_back, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2790d9d",
   "metadata": {},
   "source": [
    "> Something about base predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8630c6e",
   "metadata": {},
   "source": [
    "## HYPERPARAMETER TUNING\n",
    "Using the selected model parameters of `lookback = 12`, `n_nodes = 4`, `learning_rate = 0.001`, and `dropout = 0.1` captured the general trends of the training data, but the fit could be improved. Next I'll tune the parameters using the Optuna bayesian optimizer to find the ideal model for each well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b932c",
   "metadata": {},
   "source": [
    "#### Create Tuning Functions\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85700a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a single hyperparameter tuning trial\n",
    "def _evaluate_trial(model_type, trial, train_test_dict, well_name):\n",
    "    # Define parameter ranges to test \n",
    "    look_back = trial.suggest_categorical('look_back', [12, 24, 36])\n",
    "    n_nodes = trial.suggest_categorical('n_nodes', [4, 8, 16])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.5, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5, step=0.1)\n",
    "\n",
    "    # Get X_train and y_train datasets based on the look_back window\n",
    "    X_train = np.concatenate([train_test_dict[well_name][look_back]['X_train'] for well_name in train_test_dict.keys()], axis=0)\n",
    "    y_train = np.concatenate([train_test_dict[well_name][look_back]['y_train'] for well_name in train_test_dict.keys()], axis=0)\n",
    "\n",
    "    # Create an LSTM model instance for the selected hyperparameters\n",
    "    model = create_LSTM_model(model_type, n_nodes, look_back, learning_rate, dropout, X_train.shape[2])\n",
    "    \n",
    "    # Fit LSTM model\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=(1-SPLIT_RATIO),\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    # Define the per-well X_test and y_test datasets and predict the y_test values\n",
    "    X_test = train_test_dict[well_name][look_back]['X_test']\n",
    "    y_test = train_test_dict[well_name][look_back]['y_test']\n",
    "    y_test = np.exp(y_test) - SCALING_FACTOR\n",
    "    testPredict = model.predict(X_test, verbose=0)\n",
    "    testPredict = np.exp(testPredict) - SCALING_FACTOR\n",
    "\n",
    "    # Calculate the RMSE using the y_test and testPredict values\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, testPredict))\n",
    "\n",
    "    # Clear keras session\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    # Summarize selected hyperparameters\n",
    "    trial.set_user_attr('look_back', look_back)\n",
    "    trial.set_user_attr('n_nodes', n_nodes)\n",
    "    trial.set_user_attr('learning_rate', learning_rate)\n",
    "    trial.set_user_attr('dropout', dropout)\n",
    "\n",
    "    # Display the results of the current trial\n",
    "    print(f'(look_back={look_back}, n_nodes={n_nodes}, learning_rate={learning_rate:.4f}, dropout={dropout:.4f}) -> RMSE={test_rmse:.4f}')\n",
    "\n",
    "    return float(test_rmse)\n",
    "\n",
    "# Function to apply Bayesian hyperparameter tuning using the Optuna package\n",
    "def bayesian_tune_hyperparameters(model_type):\n",
    "    # Check if hyperparameters have already been tuned and load from directory (used to optimize re-runs of the worksheet)\n",
    "    if os.path.exists(f'tuning_rmse_results_{model_type}.csv'):\n",
    "        return pd.read_csv(f'tuning_rmse_results_{model_type}.csv')\n",
    "    else: \n",
    "        # If the hyperparameters have not been previously tuned then continue to tuning\n",
    "        trial_records = []\n",
    "        well_names = df['well_name'].unique()\n",
    "\n",
    "        # Loop over the well_names and conduct tuning trials\n",
    "        for well_name in well_names:\n",
    "            print(f'{model_type} Model Bayesian optimization trials for {well_name}:')\n",
    "            \n",
    "            # Optuna objective function to optimize\n",
    "            def objective(trial):\n",
    "                return _evaluate_trial(model_type, trial, train_test_dict, well_name)\n",
    "            \n",
    "            # Create an Optuna study instance and run tuning trials\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True, gc_after_trial=True)\n",
    "\n",
    "            # Save information on the best trial\n",
    "            best_trial = study.best_trial\n",
    "            trial_records.extend(\n",
    "                [\n",
    "                    {\n",
    "                        'well_name': well_name,\n",
    "                        'trial': trial.number,\n",
    "                        'look_back': trial.params.get('look_back'),\n",
    "                        'n_nodes': trial.params.get('n_nodes'),\n",
    "                        'learning_rate': trial.params.get('learning_rate'),\n",
    "                        'dropout': trial.params.get('dropout'),\n",
    "                        'test_rmse': trial.value,\n",
    "                    }\n",
    "                    for trial in study.trials\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a parallel coordinate plot summarizing the tuning results\n",
    "            parallel_fig = plot_parallel_coordinate(\n",
    "                study,\n",
    "                params=['look_back', 'n_nodes', 'learning_rate', 'dropout'],\n",
    "                target=lambda trial: trial.value,\n",
    "                target_name='RMSE'\n",
    "            ).update_layout(\n",
    "                title_text=f'Hyperparameter Tuning Parallel Coordinate Plot - {well_name.replace(\"_\",\" \").title()}'\n",
    "            )\n",
    "            display(parallel_fig)\n",
    "\n",
    "        # Save the RMSE scores for each trial and return a dataframe of RMSE scores\n",
    "        rmse_df = pd.DataFrame(trial_records)\n",
    "        rmse_df.to_csv(f'tuning_rmse_results_{model_type}.csv', index=False)\n",
    "        return rmse_df\n",
    "\n",
    "# Run Optuna hyperparameter tuning for the Base LSTM model\n",
    "base_rmse_df = bayesian_tune_hyperparameters(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620e641",
   "metadata": {},
   "source": [
    "> something about base HP tuning results..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f0582",
   "metadata": {},
   "source": [
    "#### Boxplot of Hyperparameter Tuning RMSE Results\n",
    "- What is this...\n",
    "- Extreme outliers in `base_rmse_df` are hidden using a +/- 1.5 * IQR filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter outliers for each well using the 1.5*IQR rule\n",
    "def rmse_iqr_filter(rmse_df):\n",
    "    quartiles = rmse_df.groupby('well_name')['test_rmse'].quantile([0.25, 0.75]).unstack()\n",
    "    quartiles.columns = ['q1', 'q3']\n",
    "    quartiles['iqr'] = quartiles['q3'] - quartiles['q1']\n",
    "    quartiles['lower'] = quartiles['q1'] - 1.5 * quartiles['iqr']\n",
    "    quartiles['upper'] = quartiles['q3'] + 1.5 * quartiles['iqr']\n",
    "    filtered_rmse_df = rmse_df.merge(quartiles[['lower', 'upper']], left_on='well_name', right_index=True)\n",
    "    filtered_rmse_df = filtered_rmse_df[(filtered_rmse_df['test_rmse'] >= filtered_rmse_df['lower']) & (filtered_rmse_df['test_rmse'] <= filtered_rmse_df['upper'])]\n",
    "    filtered_rmse_df = filtered_rmse_df.drop(columns=['lower', 'upper'])\n",
    "    return filtered_rmse_df\n",
    "\n",
    "# Function to create a set of boxplot to see the distribution of the RMSE scores attained in the hyperparameter tuning trials\n",
    "def rmse_boxplots(rmse_df, model_type):\n",
    "    # Filter rmse_df to remove outliers\n",
    "    filtered_rmse_df = rmse_iqr_filter(rmse_df)\n",
    "    \n",
    "    # Boxplot showing RMSE variation by well\n",
    "    rmse_boxplot = (\n",
    "        alt.Chart(filtered_rmse_df)\n",
    "        .mark_boxplot(size=130)\n",
    "        .encode(\n",
    "            x=alt.X('well_name:N', title='Well'),\n",
    "            y=alt.Y('test_rmse:Q', title='Test RMSE'),\n",
    "            color=alt.Color('well_name:N', legend=None)\n",
    "        )\n",
    "        .properties(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            title=alt.Title(\n",
    "                f'{model_type} Model Hyperparameter Tuning RMSE Variation by Well',\n",
    "                subtitle='Note: Extreme outliers have been hidden using a +/- 1.5 * IQR filter',\n",
    "                fontSize=20,\n",
    "                anchor='start'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return rmse_boxplot\n",
    "\n",
    "# Create boxplots summarizing the Base LSTM model's RMSE tuning results\n",
    "rmse_boxplots(base_rmse_df, 'Base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01ec8d",
   "metadata": {},
   "source": [
    "> Something about base RMSE boxplots..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bde416",
   "metadata": {},
   "source": [
    "#### Apply Best Hyperparameters\n",
    "- Hyperparameters are applied based on the best RMSE score for each well\n",
    "- Since model is being refit after hyperparameter tuning the train and test RMSE values may stochastically vary from what Optuna produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and predict an LSTM model using the best hyperparameter tuning results for each well \n",
    "def apply_best_hyperparameters(rmse_df, train_test_dict, well_name, model_type):\n",
    "    # Find the set of best hyperparameters\n",
    "    best_entry = rmse_df[rmse_df['well_name'] == well_name].sort_values('test_rmse').iloc[0]\n",
    "\n",
    "    # Extract and display the best hyperparameters\n",
    "    look_back = int(best_entry['look_back'])\n",
    "    n_nodes = int(best_entry['n_nodes'])\n",
    "    learning_rate = best_entry['learning_rate']\n",
    "    dropout = best_entry['dropout']\n",
    "    print(f'Best hyperparameters for {well_name}:\\n\\\n",
    "    look_back = {look_back}\\n\\\n",
    "    n_nodes = {n_nodes}\\n\\\n",
    "    learning_rate = {learning_rate:.4f}\\n\\\n",
    "    dropout = {dropout:.4f}'\n",
    "    )\n",
    "\n",
    "    # Create X_train and y_train datasets for the given look_back window\n",
    "    X_train = np.concatenate([train_test_dict[well_name][look_back]['X_train'] for well_name in train_test_dict.keys()], axis=0)\n",
    "    y_train = np.concatenate([train_test_dict[well_name][look_back]['y_train'] for well_name in train_test_dict.keys()], axis=0)\n",
    "\n",
    "    # Create and LSTM model instance for the given best hyperparameters\n",
    "    model = create_LSTM_model(model_type, n_nodes, look_back, learning_rate, dropout, X_train.shape[2])\n",
    "\n",
    "    # Fit the LSTM model and predict\n",
    "    history = fit_lstm(model)\n",
    "    predict_oil_production(train_test_dict, well_name, look_back, history)\n",
    "\n",
    "# Apply best base tuned hyperparameters to Well 1\n",
    "apply_best_hyperparameters(base_rmse_df, train_test_dict, 'well_1', 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best base tuned hyperparameters to Well 2\n",
    "apply_best_hyperparameters(base_rmse_df, train_test_dict, 'well_2', 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best base tuned hyperparameters to Well 3\n",
    "apply_best_hyperparameters(base_rmse_df, train_test_dict, 'well_3', 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce776053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best base tuned hyperparameters to Well 4\n",
    "apply_best_hyperparameters(base_rmse_df, train_test_dict, 'well_4', 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52616e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best base tuned hyperparameters to Well 5\n",
    "apply_best_hyperparameters(base_rmse_df, train_test_dict, 'well_5', 'base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12eba93",
   "metadata": {},
   "source": [
    "> Something about applying best HP to base..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71e30f",
   "metadata": {},
   "source": [
    "## ALTERNATIVE LSTM MODELS\n",
    "- Bidirectional LSTM\n",
    "- Stacked LSTM\n",
    "- CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3e2af",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTM\n",
    "- describe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuned Bidirectional LSTM models\n",
    "model_type = 'bidirectional'\n",
    "bidirectional_rmse_df = bayesian_tune_hyperparameters(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501bf48",
   "metadata": {},
   "source": [
    "> something about HP tuning results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32856345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display RMSE distributions\n",
    "rmse_boxplots(bidirectional_rmse_df, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6ccb6",
   "metadata": {},
   "source": [
    "> something about boxplots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Bidirectional LSTM tuned hyperparameters to Well 1\n",
    "apply_best_hyperparameters(bidirectional_rmse_df, train_test_dict, 'well_1', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668be8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Bidirectional LSTM tuned hyperparameters to Well 2\n",
    "apply_best_hyperparameters(bidirectional_rmse_df, train_test_dict, 'well_2', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Bidirectional LSTM tuned hyperparameters to Well 3\n",
    "apply_best_hyperparameters(bidirectional_rmse_df, train_test_dict, 'well_3', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Bidirectional LSTM tuned hyperparameters to Well 4\n",
    "apply_best_hyperparameters(bidirectional_rmse_df, train_test_dict, 'well_4', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Bidirectional LSTM tuned hyperparameters to Well 5\n",
    "apply_best_hyperparameters(bidirectional_rmse_df, train_test_dict, 'well_5', model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f8b43",
   "metadata": {},
   "source": [
    "> Something about best HPs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9101ff",
   "metadata": {},
   "source": [
    "#### Stacked LSTM\n",
    "- describe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e328fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuned Stacked LSTM models\n",
    "model_type = 'stacked'\n",
    "stacked_rmse_df = bayesian_tune_hyperparameters(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543aa02",
   "metadata": {},
   "source": [
    "> something about HP tuning results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74462869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display RMSE distributions\n",
    "rmse_boxplots(stacked_rmse_df, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbba177",
   "metadata": {},
   "source": [
    "> something about boxplots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Stacked LSTM tuned hyperparameters to Well 1\n",
    "apply_best_hyperparameters(stacked_rmse_df, train_test_dict, 'well_1', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef46ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Stacked LSTM tuned hyperparameters to Well 2\n",
    "apply_best_hyperparameters(stacked_rmse_df, train_test_dict, 'well_2', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d08b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Stacked LSTM tuned hyperparameters to Well 3\n",
    "apply_best_hyperparameters(stacked_rmse_df, train_test_dict, 'well_3', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Stacked LSTM tuned hyperparameters to Well 4\n",
    "apply_best_hyperparameters(stacked_rmse_df, train_test_dict, 'well_4', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3698cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best Stacked LSTM tuned hyperparameters to Well 5\n",
    "apply_best_hyperparameters(stacked_rmse_df, train_test_dict, 'well_5', model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18634f66",
   "metadata": {},
   "source": [
    "> Something about best HPs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098b209",
   "metadata": {},
   "source": [
    "#### CNN-LSTM\n",
    "- describe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e080b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuned CNN-LSTM models\n",
    "model_type = 'cnn'\n",
    "cnn_rmse_df = bayesian_tune_hyperparameters(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f874566",
   "metadata": {},
   "source": [
    "> something about HP tuning results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display RMSE distributions\n",
    "rmse_boxplots(cnn_rmse_df, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c610e",
   "metadata": {},
   "source": [
    "> something about boxplots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b076a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best CNN-LSTM tuned hyperparameters to Well 1\n",
    "apply_best_hyperparameters(cnn_rmse_df, train_test_dict, 'well_1', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best CNN-LSTM tuned hyperparameters to Well 2\n",
    "apply_best_hyperparameters(cnn_rmse_df, train_test_dict, 'well_2', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best CNN-LSTM tuned hyperparameters to Well 3\n",
    "apply_best_hyperparameters(cnn_rmse_df, train_test_dict, 'well_3', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best CNN-LSTM tuned hyperparameters to Well 4\n",
    "apply_best_hyperparameters(cnn_rmse_df, train_test_dict, 'well_4', model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best CNN-LSTM tuned hyperparameters to Well 5\n",
    "apply_best_hyperparameters(cnn_rmse_df, train_test_dict, 'well_5', model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e0251",
   "metadata": {},
   "source": [
    "> Something about best HPs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8a97b",
   "metadata": {},
   "source": [
    "## RESULTS & ANALYSIS\n",
    "- something about charting the progression of RMSE for the HP tuned models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24308ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of the RMSE dataframes for each LSTM model\n",
    "model_types_dict = {\n",
    "    'Base LSTM':          base_rmse_df,\n",
    "    'Bidirectional LSTM': bidirectional_rmse_df,\n",
    "    'Stacked LSTM':       stacked_rmse_df,\n",
    "    'CNN-LSTM':           cnn_rmse_df\n",
    "}\n",
    "\n",
    "# Find the best RMSE for each well and model type\n",
    "best_rmse_df = pd.DataFrame([])\n",
    "for model_type, df in model_types_dict.items():\n",
    "    df = df.copy()\n",
    "    df['model_type'] = model_type\n",
    "    df = df[['well_name', 'test_rmse', 'model_type']].groupby(['well_name', 'model_type'], as_index=False).min()\n",
    "    best_rmse_df = pd.concat([best_rmse_df, df])\n",
    "\n",
    "best_rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best RMSE across models\n",
    "alt.Chart(best_rmse_df).mark_line(\n",
    "        size=5,\n",
    "        strokeCap='round'\n",
    "    ).encode(\n",
    "        x=alt.X('model_type:N', title='Model Type'),\n",
    "        y=alt.Y('test_rmse:Q', title='Test RMSE'),\n",
    "        color=alt.Color('well_name:N', title='Well', scale=alt.Scale(scheme='viridis'))\n",
    "    ).properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        title=alt.Title(\n",
    "            'RMSE Progression by Well and Model Type',\n",
    "            subtitle='Note: Best hyperparameter tuned RMSE used for each model type',\n",
    "            fontSize=20,\n",
    "            anchor='start'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca572eb4",
   "metadata": {},
   "source": [
    "> Summarize RMSE results. Highlight best models types for each well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1dbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03f48dd7",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "Write a paragraph that summarizes:\n",
    "1. Re-summarize project\n",
    "2. Learning and takeaways\n",
    "3. Why something didn’t work\n",
    "4. Suggestions of ways to improve the model\n",
    "    - Apply differencing\n",
    "    - compare to other time series like ARMIA, Exponential Smoothing. or Prophet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
